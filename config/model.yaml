fingpt:
  model:
    base:
      name: "tiiuae/falcon-7b"
      format: "gguf"  # Changed from ggml to gguf
      version: "v3"
      trust_remote_code: true
      dtype: "float16"
      low_cpu_mem_usage: true
    
    llama_cpp:  # Add llama.cpp specific settings
      n_ctx: 2048
      n_threads: 4
      n_gpu_layers: 0
      n_batch: 512
      embedding: true
      f16_kv: true
      logits_all: false
      vocab_only: false
      use_mmap: true
      use_mlock: false
    
    conversion:
      output_format: "gguf"  # Update to GGUF format
      precision: "f16"
      quantization: "q4_0"
      tensor_parallel: 1
      
    inference:
      n_ctx: 2048
      n_threads: 4
      n_gpu_layers: 0
      batch_size: 1
  
  cache:
    base_dir: "%LOCALAPPDATA%/fingpt_trader"
    subdirs:
      models: "models"
      checkpoints: "checkpoints"
      tokenizers: "tokenizers"
      cache: "cache"
  
  tokenizer:
    max_length: 2048
    padding: true
    truncation: true
    
  generation:
    max_length: 128
    min_length: 1
    temperature: 0.7
    top_p: 0.9
    num_beams: 1